{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83b4cf-f176-4bee-b85f-22839dbd9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Model\n",
    "import torch\n",
    "model = Model('EleutherAI/gpt-j-6b')\n",
    "#model = Model('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b68c6-9b75-4d5e-af50-06452ebcbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d79fb2-069e-4b68-b9ed-ecbd80828347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(hs):\n",
    "    return model.lm_head(model.transformer.ln_f(hs))\n",
    "\n",
    "def get_prob_tokens(scores,topk=1):\n",
    "    probs = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    favorite_probs, favorite_tokens = probs.topk(k=topk, dim=-1)\n",
    "    return favorite_probs, favorite_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896df17-033b-4293-a9e9-26f972a6af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_logit_lens(model, tok, prefix, topk=5, color=None):\n",
    "    from baukit import show\n",
    "    num_layers = len(model.transformer.h)\n",
    "\n",
    "    with model.generate(device_map='cuda:0', max_new_tokens=3) as generator:\n",
    "        with generator.invoke(prefix) as invoker:\n",
    "            hs = []\n",
    "            for i in range(num_layers):\n",
    "                hs.append(get_scores(model.transformer.h[i].output[0]).save())\n",
    "    \n",
    "    \n",
    "    output = generator.output\n",
    "    \n",
    "    hs = [curr_hs.value for curr_hs in hs]\n",
    "    hs = torch.stack(hs)\n",
    "    print(hs.shape)\n",
    "    \n",
    "    # The full decoder head normalizes hidden state and applies softmax at the end.\n",
    "    favorite_probs, favorite_tokens = get_prob_tokens(hs, topk=topk)\n",
    "\n",
    "    # Let's also plot hidden state magnitudes\n",
    "    magnitudes = hs.norm(dim=-1)\n",
    "    \n",
    "    # For some reason the 0th token always has huge magnitudes, so normalize based on subsequent token max.\n",
    "    if (len(magnitudes[0][0]) > 1):\n",
    "        magnitudes = magnitudes / magnitudes[:,:,1:].max()\n",
    "    \n",
    "    # All the input tokens.\n",
    "    prompt_tokens = [tok.decode(t) for t in tok.encode(prefix)]\n",
    "\n",
    "    # Foreground color shows token probability, and background color shows hs magnitude\n",
    "    if color is None:\n",
    "        color = [66, 135, 245]\n",
    "    def color_fn(m, p):\n",
    "        a = [int(255 * (1-m) + c * m) for c in color]\n",
    "        b = [int(255 * (1-p) + 0 * p)] * 3\n",
    "        return show.style(background=f'rgb({a[0]}, {a[1]}, {a[2]})',\n",
    "                          color=f'rgb({b[0]}, {b[1]}, {b[2]})' )\n",
    "\n",
    "    # In the hover popup, show topk probabilities beyond the 0th.\n",
    "    def hover(tok, prob, toks, m):\n",
    "        lines = [f'mag: {m:.2f}']\n",
    "        for p, t in zip(prob, toks):\n",
    "            lines.append(f'{tok.decode(t)}: prob {p:.2f}')\n",
    "        return show.attr(title='\\n'.join(lines))\n",
    "    \n",
    "    # Construct the HTML output using show.\n",
    "    header_line = [ # header line\n",
    "             [[show.style(fontWeight='bold'), 'Layer']] +\n",
    "             [\n",
    "                 [show.style(background='orange'), show.attr(title=f'Token {i}'), t]\n",
    "                 for i, t in enumerate(prompt_tokens)\n",
    "             ]\n",
    "         ]\n",
    "    layer_logits = [\n",
    "             # first column\n",
    "             [[show.style(fontWeight='bold'), layer]] +\n",
    "             [\n",
    "                 # subsequent columns\n",
    "                 [color_fn(m, p[0]), hover(tok, p, t, m), show.style(overflowX='hide'), tok.decode(t[0])]\n",
    "                 for m, p, t in zip(wordmags, wordprobs, words)\n",
    "             ]\n",
    "        for layer, wordmags, wordprobs, words in\n",
    "                zip(range(len(magnitudes)), magnitudes[:, 0], favorite_probs[:, 0], favorite_tokens[:,0])]\n",
    "    \n",
    "    # If you want to get the html without showing it, use show.html(...)\n",
    "    show(header_line + layer_logits + header_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5973b-771f-4ab1-965d-34f01df20cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(model, model.tokenizer, '/* Copyright (C)', topk=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3fde1-e708-4b3e-86bd-e5f88996f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_layer_scores():\n",
    "\n",
    "    hs = model.transformer.h[-1].output[0]\n",
    "    return model.lm_head(model.transformer.ln_f(hs))\n",
    "\n",
    "\n",
    "def decode(scores):\n",
    "    print(len(scores))\n",
    "    scores = scores.argmax(dim=2)[0, -1]\n",
    "    return model.tokenizer.decode(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d59c52-a2ab-4e6e-b4dd-6f8520401120",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = len(model.transformer.h)\n",
    "num_toks_gen = 3\n",
    "\n",
    "with model.generate(device_map='cuda:0', max_new_tokens=3) as generator:\n",
    "    with generator.invoke('Madison square garden is located in the city of New') as invoker:\n",
    "        tokenized = invoker.tokens\n",
    "        init_logits = get_last_layer_scores().save()\n",
    "        next_tok_logits =  []\n",
    "        for i in range(1, num_toks_gen):\n",
    "            invoker.next()\n",
    "            next_tok_logits.append(get_last_layer_scores().save())\n",
    "\n",
    "output = generator.output\n",
    "init_logits = init_logits.value\n",
    "next_toks = []\n",
    "for next_tok in next_tok_logits:\n",
    "    init_logits = torch.cat((init_logits, next_tok.value), dim=1)\n",
    "\n",
    "pred, fav_toks = get_prob_tokens(init_logits)\n",
    "for ft in fav_toks[0]:\n",
    "    print(model.tokenizer.decode(ft))\n",
    "print(output.shape)\n",
    "print(init_logits.shape)\n",
    "# for p, t in zip(pred, fav_toks[0]):\n",
    "#     print(f'{model.tokenizer.decode(t)}: prob {p:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a960abc-08b8-4dfc-911f-75c1f8c07ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_logit_lens_extended(model, tok, prefix, topk=5, num_toks_gen=3, color=None):\n",
    "    from baukit import show\n",
    "    num_layers = len(model.transformer.h)\n",
    "    \n",
    "    with model.generate(device_map='cuda:0', max_new_tokens=num_toks_gen) as generator:\n",
    "        with generator.invoke(prefix) as invoker:\n",
    "            tokenized = invoker.tokens\n",
    "            init_logits = []\n",
    "            for i in range(num_layers):\n",
    "                init_logits.append(get_scores(model.transformer.h[i].output[0]).save())\n",
    "            next_tok_logits =  []\n",
    "            for _ in range(1, num_toks_gen):\n",
    "                invoker.next()\n",
    "                curr_tok_logits = []\n",
    "                for i in range(num_layers):\n",
    "                    curr_tok_logits.append(get_scores(model.transformer.h[i].output[0]).save())\n",
    "                next_tok_logits.append(curr_tok_logits)\n",
    "    \n",
    "    output = generator.output\n",
    "\n",
    "    hs = [curr_hs.value for curr_hs in init_logits]\n",
    "    hs = torch.stack(hs)\n",
    "\n",
    "    next_hs = []\n",
    "    for next_curr_hs in next_tok_logits:\n",
    "        next_sub_hs = [curr_hs.value for curr_hs in next_curr_hs]\n",
    "        next_sub_hs = torch.stack(next_sub_hs)\n",
    "        hs = torch.cat((hs, next_sub_hs), dim=2)\n",
    "    \n",
    "    # The full decoder head normalizes hidden state and applies softmax at the end.\n",
    "    favorite_probs, favorite_tokens = get_prob_tokens(hs, topk=topk)\n",
    "    # Let's also plot hidden state magnitudes\n",
    "    magnitudes = hs.norm(dim=-1)\n",
    "    \n",
    "    # For some reason the 0th token always has huge magnitudes, so normalize based on subsequent token max.\n",
    "    if (len(magnitudes[0][0]) > 1):\n",
    "        magnitudes = magnitudes / magnitudes[:,:,1:].max()\n",
    "    \n",
    "    # All the input tokens.\n",
    "    prompt_tokens = [tok.decode(t) for t in output[0,:-1]]\n",
    "\n",
    "    # Foreground color shows token probability, and background color shows hs magnitude\n",
    "    if color is None:\n",
    "        color = [66, 135, 245]\n",
    "    def color_fn(m, p):\n",
    "        a = [int(255 * (1-m) + c * m) for c in color]\n",
    "        b = [int(255 * (1-p) + 0 * p)] * 3\n",
    "        return show.style(background=f'rgb({a[0]}, {a[1]}, {a[2]})',\n",
    "                          color=f'rgb({b[0]}, {b[1]}, {b[2]})' )\n",
    "\n",
    "    # In the hover popup, show topk probabilities beyond the 0th.\n",
    "    def hover(tok, prob, toks, m):\n",
    "        lines = [f'mag: {m:.2f}']\n",
    "        for p, t in zip(prob, toks):\n",
    "            lines.append(f'{tok.decode(t)}: prob {p:.2f}')\n",
    "        return show.attr(title='\\n'.join(lines))\n",
    "    \n",
    "    # Construct the HTML output using show.\n",
    "    header_line = [ # header line\n",
    "             [[show.style(fontWeight='bold'), 'Layer']] +\n",
    "             [\n",
    "                 [show.style(background='orange'), show.attr(title=f'Token {i}'), t]\n",
    "                 for i, t in enumerate(prompt_tokens)\n",
    "             ]\n",
    "         ]\n",
    "    layer_logits = [\n",
    "             # first column\n",
    "             [[show.style(fontWeight='bold'), layer]] +\n",
    "             [\n",
    "                 # subsequent columns\n",
    "                 [color_fn(m, p[0]), hover(tok, p, t, m), show.style(overflowX='hide'), tok.decode(t[0])]\n",
    "                 for m, p, t in zip(wordmags, wordprobs, words)\n",
    "             ]\n",
    "        for layer, wordmags, wordprobs, words in\n",
    "                zip(range(len(magnitudes)), magnitudes[:, 0], favorite_probs[:, 0], favorite_tokens[:,0])]\n",
    "    \n",
    "    # If you want to get the html without showing it, use show.html(...)\n",
    "    show(header_line + layer_logits + header_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933573e3-6fb5-4da5-923b-5eefa6743eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens_extended(model, model.tokenizer, 'Madison square garden is located in the city of New', topk=5, num_toks_gen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692a301-3250-4adc-b72f-628b8c322d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prefix(path):\n",
    "    prefix_vector = torch.load(path)\n",
    "    return prefix_vector.to(torch.float32)\n",
    "    \n",
    "#context_vector = load_prefix(\"/disk/u/koyena/PrefixLens/results/training/layer13to4_tk1/soft_prefix.pt\")\n",
    "context_vector = load_prefix(\"/disk/u/koyena/PrefixLens/results/conll_training/layer13to27_tk1/soft_prefix.pt\")\n",
    "print(context_vector.shape)\n",
    "# context_vector = context_vector.unsqueeze(0)\n",
    "#print(context_vector.shape)\n",
    "context_vector = context_vector.expand(1, -1, -1)\n",
    "print(context_vector.shape)\n",
    "context_vector.to(\"cuda:0\")\n",
    "#inputs_embeds = 1.00 * context_vector\n",
    "#inputs_embeds.to(\"cuda:0\")\n",
    "#print(inputs_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8bb41-a2a2-4b2e-8218-cfaf535ee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_future_lens(model, tok, prefix, context, in_layer = 13, tgt_in_layer = 4, topk=5, num_toks_gen=3, color=None):\n",
    "    from baukit import show\n",
    "\n",
    "    prefix_pos = len(tok(prefix)['input_ids']) - 1\n",
    "    \n",
    "    context = context.detach()\n",
    "    print(context)\n",
    "    # prefix_pos = -1\n",
    "    context_pos = 9\n",
    "    #context_pos = len(tok(context)['input_ids']) - 1\n",
    "    num_layers = len(model.transformer.h)\n",
    "\n",
    "    with model.generate(device_map='cuda:0', max_new_tokens=num_toks_gen) as generator:\n",
    "        with generator.invoke(prefix) as invoker:\n",
    "            context_tokenized = invoker.tokens\n",
    "            transplant_hs = model.transformer.h[in_layer].output[0].t[prefix_pos].save()\n",
    "            overall_hs = []\n",
    "            init_logits = []\n",
    "            for i in range(num_layers):\n",
    "                init_logits.append(get_scores(model.transformer.h[i].output[0]).save())\n",
    "                overall_hs.append(model.transformer.h[i].output[0].save())\n",
    "    \n",
    "    output = generator.output\n",
    "    print(tok.decode(output[0][prefix_pos+1:]))\n",
    "    first_set_logits = [curr_hs.value for curr_hs in init_logits]\n",
    "    hs = [curr_hs.value for curr_hs in overall_hs]\n",
    "    first_set_logits = torch.stack(first_set_logits)\n",
    "    counter = 0\n",
    "    future_outputs = []\n",
    "    future_preds = []\n",
    "    for curr_hs in hs:\n",
    "        curr_future_outputs = []\n",
    "        curr_future_preds = []\n",
    "        for x in range(0, curr_hs.shape[1]):\n",
    "            sub_hs = curr_hs[:,x,:][None,:]\n",
    "            with model.generate(device_map='cuda:0', max_new_tokens=num_toks_gen) as generator2:\n",
    "                with generator2.invoke(\"_ _ _ _ _ _ _ _ _ _\") as invoker:\n",
    "                    #model.transformer.wte.output = context.detach().to('cuda')\n",
    "                    model.transformer.wte.output = context\n",
    "                    model.transformer.h[tgt_in_layer].output[0].t[context_pos] = sub_hs\n",
    "                    invoker.next()\n",
    "                    future_output_logits = model.lm_head.output.save()\n",
    "                    invoker.next()\n",
    "                    future_output_logits_next = model.lm_head.output.save()\n",
    "                    invoker.next()\n",
    "                    future_output_logits_next_next = model.lm_head.output.save()\n",
    "            counter+= 1\n",
    "            #curr_output = generator2.output\n",
    "            #print(curr_output)\n",
    "            curr_output = torch.squeeze(future_output_logits.value,0)\n",
    "            curr_output_next = torch.squeeze(future_output_logits_next.value,0)\n",
    "            curr_output_next_next = torch.squeeze(future_output_logits_next_next.value,0)\n",
    "            #print(\"CURR OUTPUT\", curr_output.shape)\n",
    "            curr_fav_tok_pred, curr_fav_tok = get_prob_tokens(curr_output[0], topk=1)\n",
    "            curr_fav_tok_pred_next, curr_fav_tok_next = get_prob_tokens(curr_output_next[0], topk=1)\n",
    "            curr_fav_tok_pred_next_next, curr_fav_tok_next_next = get_prob_tokens(curr_output_next_next[0], topk=1)\n",
    "            #print(curr_fav_tok)\n",
    "            curr_future_outputs.append([tok.decode(curr_fav_tok),tok.decode(curr_fav_tok_next),tok.decode(curr_fav_tok_next_next)])\n",
    "            curr_future_preds.append([curr_fav_tok_pred[0].item(), curr_fav_tok_pred_next[0].item(), curr_fav_tok_pred_next_next[0].item()])\n",
    "        future_outputs.append(curr_future_outputs)\n",
    "        future_preds.append(curr_future_preds)\n",
    "    # The full decoder head normalizes hidden state and applies softmax at the end.\n",
    "    favorite_probs, favorite_tokens = get_prob_tokens(first_set_logits, topk=topk)\n",
    "    # # Let's also plot hidden state magnitudes\n",
    "    # magnitudes = first_set_logits.norm(dim=-1)\n",
    "    \n",
    "    # # For some reason the 0th token always has huge magnitudes, so normalize based on subsequent token max.\n",
    "    # if (len(magnitudes[0][0]) > 1):\n",
    "    #     magnitudes = magnitudes / magnitudes[:,:,1:].max()\n",
    "    \n",
    "    # All the input tokens.\n",
    "    prompt_tokens = [tok.decode(t) for t in tok.encode(prefix)]\n",
    "\n",
    "    # Foreground color shows token probability, and background color shows hs magnitude\n",
    "    if color is None:\n",
    "        #color = [66, 135, 245]\n",
    "        color = [50, 168, 123]\n",
    "    def color_fn(p, future_probs = None):\n",
    "        a = [int(255 * (1-p) + c * p) for c in color]\n",
    "        if future_probs is not None:\n",
    "            total_probs = p + sum(future_probs)\n",
    "            new_p = total_probs / (len(future_probs) + 1)\n",
    "            a = [int(255 * (1-new_p) + c * new_p) for c in color]\n",
    "        return show.style(background=f'rgb({a[0]}, {a[1]}, {a[2]})')\n",
    "\n",
    "    # In the hover popup, show topk probabilities beyond the 0th.\n",
    "    def hover(tok, prob, toks):\n",
    "        lines = []\n",
    "        for p, t in zip(prob, toks):\n",
    "            lines.append(f'{tok.decode(t)}: prob {p:.2f}')\n",
    "        return show.attr(title='\\n'.join(lines))\n",
    "\n",
    "    def decode_escape(tok,token,actual_decode=True):\n",
    "        if not actual_decode:\n",
    "            if type(token) == list:\n",
    "                return [t.encode(\"unicode_escape\").decode() for t in token]\n",
    "            return token.encode(\"unicode_escape\").decode()\n",
    "        if type(token) == list:\n",
    "                return [tok.decode(t).encode(\"unicode_escape\").decode() for t in token]\n",
    "        return tok.decode(token).encode(\"unicode_escape\").decode()\n",
    "    # Construct the HTML output using show.\n",
    "        #[[show.style(fontWeight='bold'), 'Layer/Input']] +\n",
    "    # background=f'rgb(247, 212, 200)'\n",
    "    header_line = [ # header line\n",
    "                [[' ']] + \n",
    "             [\n",
    "                 [show.style(fontWeight='bold', width='50px'), show.attr(title=f'Token {i}'), t]\n",
    "                 for i, t in enumerate(prompt_tokens)\n",
    "             ]\n",
    "         ]\n",
    "    layer_logits = [\n",
    "             # first column\n",
    "             # [[show.style(fontWeight='bold', width='50px'), f'L{layer}']] +\n",
    "             [\n",
    "                 # subsequent columns\n",
    "                 [color_fn(p[0], fprobs), hover(tok, p, t), show.style(overflowX='hide'), f\"{decode_escape(tok, t[0])}{''.join(decode_escape(tok, ft, False))}\"]\n",
    "                 for p, t, ft, fprobs in zip(wordprobs, words, future_words, future_probs)\n",
    "             ]\n",
    "        for layer, wordprobs, words, future_words, future_probs in\n",
    "                zip(range(len(favorite_probs[:, 0])), favorite_probs[:, 0], favorite_tokens[:,0], future_outputs, future_preds)]\n",
    "    \n",
    "    # If you want to get the html without showing it, use show.html(...)\n",
    "    # show(header_line + layer_logits + header_line)\n",
    "    # print(show.html(header_line + layer_logits + header_line))\n",
    "\n",
    "    show(layer_logits)\n",
    "    print(show.html(layer_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22e02a-2708-458c-83a8-47188e662ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Madison square garden is located in the city of New'\n",
    "#prefix = \"\"\"As is known in the art, it is frequently desirable to detect and segment an object from a background of other objects and/or from a background of noise. One application, for example, is in MRI where it is desired to segment an anatomical feature of a human patient, such as, for example, a vertebra of the patent. In other cases it would be desirable to segment a moving, deformable anatomical feature such as the heart. In 1988, Osher and Sethian, in a paper entitled “Fronts propagation with curvature dependent speed: Algorithms based on Hamilton-Jacobi formulations” J. of Comp. Phys., 79:12-49, 1988, introduced the level set method, it being noted that a precursor of the level set method was proposed by Dervieux and Thomasset in a paper entitled “A finite element method for the simulation of Raleigh-Taylor instability”. Springer Lect. Notes in Math., 771:145-158, 1979, as a means to implicitly propagate hypersurfaces C(t) in a domain Ω⊂Rn by evolving an appropriate\"\"\"\n",
    "#prefix = \"Bill Nelson studied at University\"\n",
    "#context = '���chargedansionFollowing SlipUFC gallery RugOURsn'\n",
    "#context = \"Hello! Could you please tell me more about\"\n",
    "show_future_lens(model, \n",
    "                 model.tokenizer, prefix, context_vector, topk=5, in_layer = 13, tgt_in_layer = 13, num_toks_gen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6b51f-2195-4d50-8194-65a9398d4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Marty McFly from'\n",
    "show_future_lens(model, \n",
    "                 model.tokenizer, prefix, context_vector, topk=5, in_layer = 13, tgt_in_layer = 13, num_toks_gen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8c8ea-5955-4763-be58-54930ab80de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'Marty McFly from'\n",
    "show_future_lens(model, \n",
    "                 model.tokenizer, prefix, context_vector, topk=5, in_layer = 13, tgt_in_layer = 13, num_toks_gen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970b0bc-8362-481c-9454-0bb37e133c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d1fa8-138c-446d-b930-abd82dd570b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Bill Nelson studied at University\"\n",
    "show_future_lens(model, \n",
    "                 model.tokenizer, prefix, context_vector, topk=5, in_layer = 13, tgt_in_layer = 4, num_toks_gen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7e649-eff2-4449-b113-41e342052591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_logit_lens(model, model.tokenizer, \"+ + + + + + + + + +\", topk=5, color=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dbfb64-b53d-48fb-9f9e-8ba264691fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    context_vector = context_vector\n",
    "\n",
    "print(\"initial_context_vector\", context_vector.detach())\n",
    "\n",
    "with model.generate(device_map='cuda:0', max_new_tokens=3) as generator:\n",
    "    \n",
    "    with generator.invoke(\"Madison square garden is located in the city of New\") as invoker:\n",
    "        #model.transformer.wte.output = context_vector.detach().to('cuda')\n",
    "        model.transformer.wte.output = context_vector\n",
    "        embeddings = model.transformer.wte.output.save()\n",
    "\n",
    "        logits = model.lm_head.output.save()\n",
    "\n",
    "logits = logits.value.argmax(dim=-1)[0]\n",
    "\n",
    "print(\"context vector\", context_vector.detach().to('cuda'))\n",
    "print(\"embeddings\", embeddings.value)\n",
    "\n",
    "\n",
    "\n",
    "with model.generate(device_map='cuda:0', max_new_tokens=3) as generator:\n",
    "\n",
    "    with generator.invoke(\"_ _ _ _ _ _ _ _ _ _\") as invoker:\n",
    "\n",
    "        model.transformer.wte.output = embeddings.value\n",
    "\n",
    "        future_output_logits = model.lm_head.output.save()\n",
    "\n",
    "        logits = model.lm_head.output.save()\n",
    "\n",
    "logits = logits.value.argmax(dim=-1)[0]\n",
    "\n",
    "print(model.tokenizer.decode(logits[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd1e24-d12e-4f52-baf8-e74186b93d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(model, model.tokenizer, 'Marty McFly from', topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3863d-ccb6-4de6-b8e1-90fa33613f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
