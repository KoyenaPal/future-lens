{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from customized import CustomizedGPTJForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6b were not used when initializing CustomizedGPTJForCausalLM: ['transformer.h.0.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.13.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.10.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.4.attn.bias', 'transformer.h.14.attn.bias', 'transformer.h.6.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.2.attn.bias', 'transformer.h.17.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.15.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.20.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.18.attn.bias', 'transformer.h.0.attn.bias', 'transformer.h.6.attn.masked_bias']\n",
      "- This IS expected if you are initializing CustomizedGPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomizedGPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomizedGPTJForCausalLM were not initialized from the model checkpoint at EleutherAI/gpt-j-6b and are newly initialized: ['transformer.prefix_embedding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model = CustomizedGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\", torch_dtype=torch.bfloat16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\", padding_side=\"left\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Information Unveilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[39m.\u001b[39munprefix_model()\n\u001b[1;32m     25\u001b[0m original_hidden_state \u001b[39m=\u001b[39m model(input_ids, return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mhidden_states\n\u001b[0;32m---> 26\u001b[0m \u001b[39mlen\u001b[39;49m(original_hidden_state)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Threshold for confidence score of the generated content\n",
    "threshold_confidence = 0.5\n",
    "\n",
    "# The maximum generated token from the hidden state\n",
    "max_length = 5\n",
    "\n",
    "# Sub-layer in [0, 27] to extract hidden state from\n",
    "sublayer = 21\n",
    "\n",
    "# Specify the prefix that is best at unveiling the information at the subsequent n token\n",
    "best_prediction_at = 1\n",
    "\n",
    "# Utterance to extract hidden state from, will always extract the last token hidden states\n",
    "utterance = \"Specifically, we train GPT-3, an\"\n",
    "\n",
    "# Loading the corresponding trained prefix \n",
    "model.load_prefix(f\"./results/training/layer{sublayer}to27_tk{best_prediction_at}/soft_prefix.pt\")\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the utterance\n",
    "input_ids = tokenizer(utterance, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# Get the original hidden state from the sublayer\n",
    "model.unprefix_model()\n",
    "original_hidden_state = model(input_ids, output_hidden_states=True).hidden_states[sublayer][:, -1, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Information Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
